{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data preprocessing \n",
    "### load raw files -> generate clips -> compute features-> aggregate into a matrix of features and scores\n",
    "** Pandas version required to load pickle files is 0.20.1 or greater **\n",
    "\n",
    "* Try classifying symptom presence from \"typing\", \"walking\" and \"finger-to-nose\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import pickle #to save files\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import butter, welch, filtfilt\n",
    "\n",
    "# from PreprocessFcns import gen_clips, powerspectra\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#-- For interactive plots--\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#---Pandas version required to load pickle files is 0.20.1 or greater---\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Set path to folder containing Subject ID numbers\n",
    "# path = '/Volumes/RTO/CIS-PD Study/Subjects/' #Mac\n",
    "path = r'X:\\CIS-PD Study\\Subjects' #Windows remote path\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "folder_path = r'X:\\CIS-PD Study' #generic Windows repo path\n",
    "\n",
    "#Path where dictionary subject data is stored\n",
    "dict_path = 'X:\\CIS-PD Study\\Data_dict' #remote repo\n",
    "# dict_path = '../Data_dict' # Mac local path\n",
    "# dict_path = r'C:\\Users\\adai\\Documents\\Data_dict' #Windows local path adai\n",
    "\n",
    "scores_path = r'X:\\CIS-PD Study\\Scores' #remote repo\n",
    "# scores_path = '../Scores/' # Mac local path\n",
    "\n",
    "#path where feature matrix is saved\n",
    "features_path = r'X:\\CIS-PD Study\\FeatureMatrix' #remote repo\n",
    "# features_path = '../FeatureMatrix' # Mac local path\n",
    "# features_path = r'C:\\Users\\adai\\Documents\\FeatureMatrix' #Windows local path adai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete = list(['Heart Rate Variability', 'MDS-UPDRS #1: Finger Tapping',\n",
    "           'MDS-UPDRS #2: Hand Movements', 'MDS-UPDRS #3: Pronation-Supination',\n",
    "           'MDS-UPDRS #4: Toe Tapping', 'MDS-UPDRS #5: Leg Agility',\n",
    "           'MDS-UPDRS #6: Arising from Chair', 'MDS-UPDRS #7: Gait',\n",
    "           'MDS-UPDRS #8: Postural Stability', 'MDS-UPDRS #9: Postural Hand Tremor',\n",
    "           'MDS-UPDRS #10: Kinetic Hand Tremor', 'MDS-UPDRS #11: Rest Tremor',\n",
    "           'Motor #1: Standing', 'Motor #2: Walking', 'Motor #3: Walking while Counting',\n",
    "           'Motor #4: Finger to Nose', 'Motor #5: Alternating Hand Movements',\n",
    "           'Motor #6: Sit to Stand', 'Motor #7: Drawing on Paper',\n",
    "           'Motor #8: Typing on a Computer', 'Motor #9: Nuts and Bolts',\n",
    "           'Motor #10: Drinking Water', 'Motor #11: Organizing Folder',\n",
    "           'Motor #12: Folding Towels', 'Motor #13: Sitting'])\n",
    "\n",
    "complete_temp = complete\n",
    "complete_temp.remove('MDS-UPDRS #11: Rest Tremor')\n",
    "\n",
    "\n",
    "def process_annotations(path):\n",
    "#def process_annotations(path, SubID):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Processes raw annotations file to extract start / end timestamps and remove unnecessary data\n",
    "#\n",
    "# Inputs:  path - filepath of the subject folder containing annotations.csv\n",
    "#\n",
    "# Outputs: df - dataframe containing list of activities and their start / end timestamps\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    df = pd.read_csv(os.path.join(path, 'annotations.csv'))\n",
    "    del df['Timestamp (ms)']\n",
    "    del df['AnnotationId']\n",
    "    del df['AuthorId']\n",
    "    \n",
    "    testInfo = df[df.EventType == 'Testing Day'].dropna(how='any', axis=0)\n",
    "    testInfo['Start Timestamp (ms)'] = pd.to_datetime(testInfo['Start Timestamp (ms)'], unit='ms', utc=True).dt.tz_localize('UTC').dt.tz_convert('US/Central')\n",
    "    del testInfo['Stop Timestamp (ms)']\n",
    "    del testInfo['EventType']\n",
    "    del df['Value']\n",
    "    \n",
    "    testInfo = testInfo.rename(columns = {'Value':'Day', 'Start Timestamp (ms)':'Date'}).reset_index(drop=True)\n",
    "    testInfo['Date'] = testInfo['Date'].dt.date\n",
    "    \n",
    "    df = df[(df.EventType != 'Testing Day')]\n",
    "    \n",
    "    sorter = set(df.EventType.unique().flatten())\n",
    "    sorterIndex = dict(zip(sorter, range(len(sorter))))\n",
    "        \n",
    "    df['EventType_Rank'] = df['EventType'].map(sorterIndex)\n",
    "    df['Cycle'] = df.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    del df['EventType_Rank']\n",
    "    df[df['EventType'].str.contains('Heart')] = df[df['EventType'].str.contains('Heart')].replace(to_replace={'Cycle': {1: 'NaN', 2: 'NaN', 3: 'NaN', 4: 'NaN'}})\n",
    "    df = df.reset_index(drop=True).set_index('EventType')\n",
    "    \n",
    "    # return d1_df, d2_df, df\n",
    "    return df, testInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper fcns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_data(SubID, path):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# For a given subject, extracts and separates accelerometer, gyroscope, and EMG/ECG data into trials and sensor per activity\n",
    "#\n",
    "# Inputs: SubID - string of numbers corresponding to the subject ID\n",
    "#         path - system path to corresponding subject's raw data files\n",
    "#\n",
    "# Outputs: act_dict - dictionary of both MDS-UPDRS and Motor Assessment activities separated by trial, sensor location, and\n",
    "#                     accelerometer + gyroscope or accelerometer + EMG/ECG data. Every key within this dictionary is a dictionary\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    #timestamps = process_annotations(path, SubID)\n",
    "    timestamps, testInfo = process_annotations(path)\n",
    "    timestamps_new = fix_errors(SubID, timestamps)\n",
    "    del timestamps\n",
    "    \n",
    "    # Creates list of sensor locations from folders within subject's raw data directory\n",
    "    locations = [locs for locs in os.listdir(path) if os.path.isdir(os.path.join(path, locs))]\n",
    "    \n",
    "    # Creates dictionary of empty dataframes to merge all accelerometer, gyroscope, and EMG/ECG data for each sensor\n",
    "    accel = {locs: pd.DataFrame() for locs in locations}\n",
    "    gyro = {locs: pd.DataFrame() for locs in locations}\n",
    "    elec = {locs: pd.DataFrame() for locs in locations}\n",
    "    \n",
    "    # Finds and merges all accelerometer, gyroscope, and EMG/ECG data for each sensor, retains datetime information\n",
    "    for root, dirs, files in os.walk(path, topdown=True):\n",
    "        for filenames in files:\n",
    "            if filenames.endswith('accel.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                accel[location] = accel[location].append(temp_df)\n",
    "\n",
    "            elif filenames.endswith('gyro.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                gyro[location] = gyro[location].append(temp_df)\n",
    "\n",
    "            elif filenames.endswith(('elec.csv', 'emg.csv', 'ecg.csv', 'ekg.csv')):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                elec[location] = elec[location].append(temp_df)\n",
    "                \n",
    "    # Temporary fix for missing activities, added 10/10/2017 will remove once error functions are finished\n",
    "    if SubID in ('1024', '1030', '1032'):\n",
    "        complete_acts = complete_temp\n",
    "    else:\n",
    "        complete_acts = complete\n",
    "                \n",
    "    # Complete dictionary of all activities\n",
    "    act_dict = {acts: pd.DataFrame() for acts in complete_acts}\n",
    "\n",
    "    # Populate dictionary keys per activity with every iteration / trial\n",
    "    for activities in complete_acts:\n",
    "        \n",
    "        startTimestamp = timestamps_new.loc[activities, 'Start Timestamp (ms)'].values\n",
    "        endTimestamp = timestamps_new.loc[activities, 'Stop Timestamp (ms)'].values\n",
    "\n",
    "        # Create trial dictionary with each key containing all sensor data related with each activity's trial\n",
    "        trial_dict = {trials: pd.DataFrame() for trials in range(0, len(startTimestamp))}\n",
    "\n",
    "        # Populate trial directory keys\n",
    "        for trials in range(0, len(startTimestamp)):\n",
    "\n",
    "            startTime = startTimestamp[trials]\n",
    "            endTime = endTimestamp[trials]\n",
    "\n",
    "            # Create sensor location dictionary with each key corresponding to sensor locations\n",
    "            sensor_dict = {locs: pd.DataFrame() for locs in locations}\n",
    "\n",
    "            # Extract sensor data and populate sensor_dict with sensor data\n",
    "            for location in locations:\n",
    "\n",
    "                data = {'accel': pd.DataFrame(), 'gyro': pd.DataFrame(), 'elec': pd.DataFrame()}\n",
    "\n",
    "                if not accel[location].empty:\n",
    "                    accelData = accel[location]\n",
    "                    data['accel'] = accelData[(accelData.index >= startTime) & (accelData.index <= endTime)]\n",
    "\n",
    "                if not gyro[location].empty:\n",
    "                    gyroData = gyro[location]\n",
    "                    data['gyro'] = gyroData[(gyroData.index >= startTime) & (gyroData.index <= endTime)]\n",
    "\n",
    "                if not elec[location].empty:\n",
    "                    elecData = elec[location]\n",
    "                    data['elec'] = elecData[(elecData.index >= startTime) & (elecData.index <= endTime)]\n",
    "\n",
    "                sensor_dict[location] = data\n",
    "\n",
    "            trial_dict[trials] = sensor_dict\n",
    "\n",
    "        act_dict[activities] = trial_dict\n",
    "    \n",
    "    return act_dict, timestamps_new, testInfo\n",
    "\n",
    "\n",
    "                                \n",
    "def plot_data(acts, activity, trial, sensor, data, start=0, end=100000):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Plots requested data\n",
    "# \n",
    "# Inputs: acts - activity dataframe containing all sensor data from one subject\n",
    "#         activity - desired activity to plot\n",
    "#         trial - desired trial number to plot\n",
    "#         sensor - desired sensor (serial number or name of location) to analyze\n",
    "#         data - desired type of data to analyze (accel, gyro, EMG/ECG)\n",
    "#         start - starting index, default starts at first point\n",
    "#         end - ending index, default is 500th data point\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    toPlot = pd.DataFrame(acts[activity][trial]).loc[data, sensor][start:end].plot(figsize=(6,4))\n",
    "    \n",
    "\n",
    "#extract clips for accelerometer and gyro data\n",
    "def gen_clips(act_dict,task,location,clipsize=5000,overlap=0,verbose=False,startTS=0,endTS=1):\n",
    "    \n",
    "    clip_data = {} #the dictionary with clips\n",
    "    #params\n",
    "    len_tol = 0.8   #% of the intended clipsize below which clip is not used\n",
    "\n",
    "    for trial in act_dict[task].keys():\n",
    "        clip_data[trial] = {}            \n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            #choose to create clips only on a fraction of the data (0<[startTS,endTS]<1)\n",
    "            if (startTS > 0) | (endTS < 1):\n",
    "                rawdata = rawdata.iloc[round(startTS*len(rawdata)):round(endTS*len(rawdata)),:]\n",
    "                #reindex time (relative to start)\n",
    "                idx = rawdata.index\n",
    "                idx = idx-idx[0]\n",
    "                rawdata.index = idx\n",
    "            #create clips data\n",
    "            deltat = np.median(np.diff(rawdata.index))\n",
    "            idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "            clips = []\n",
    "            for i in idx:\n",
    "                c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                if len(c) > 0.8*int(clipsize/deltat): #discard clips whose length is less than len_tol% of the window size\n",
    "                    clips.append(c)\n",
    "            clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] #store the length of each clip\n",
    "            #assemble in dict\n",
    "            clip_data[trial][s] = {'data':clips, 'clip_len':clip_len}\n",
    "\n",
    "    return clip_data\n",
    "\n",
    "\n",
    "#returns power spectra of the signal over each channel between min and max freq at given resolution (nbins)\n",
    "#returns the labels for each bin\n",
    "#if binavg is True it averages the PSD within bins to reduce PSD noise\n",
    "def powerspectra(x,fm,fM,nbins=10,relative=False,binavg=True):\n",
    "    \n",
    "    #feature labels\n",
    "    labels=[]\n",
    "    s = np.linspace(fm,fM,nbins)\n",
    "    lax = ['X','Y','Z']\n",
    "    for l in lax:\n",
    "        for i in s:\n",
    "            labels.append('fft'+l+str(int(i)))\n",
    "            \n",
    "    #signal features\n",
    "    n = len(x) #number of samples in clip\n",
    "    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "    timestep = 1/Fs \n",
    "    freq = np.fft.fftfreq(n,d=timestep) #frequency bins\n",
    "\n",
    "    #run FFT on each channel \n",
    "    Xf = x.apply(np.fft.fft)\n",
    "    Xf.index = np.round(freq,decimals=1) #reindex w frequency bin\n",
    "    Pxx = Xf.apply(np.abs)\n",
    "    Pxx = Pxx**2 #power spectra\n",
    "    if relative:\n",
    "        Pxx = Pxx/np.sum(Pxx,axis=0) #power relative to total\n",
    "    \n",
    "    #power spectra between fm-fM Hz\n",
    "    bin1 = int(timestep*n*fm)\n",
    "    bin2 = int(timestep*n*fM)\n",
    "    bins = np.linspace(bin1,bin2,nbins,dtype=int)\n",
    "#     print(bins/(round(timestep*n)))\n",
    "\n",
    "    #average power spectra within bins\n",
    "    if binavg:\n",
    "        deltab = int(0.5*np.diff(bins)[0]) #half the size of a bin (in samples)\n",
    "        Pxxm = []\n",
    "        for i in bins:\n",
    "            start = int(max(i-deltab,bins[0]))\n",
    "            end = int(min(i+deltab,bins[-1]))\n",
    "            Pxxm.append(np.mean(Pxx.iloc[start:end,:].values,axis=0))            \n",
    "        Pxxm = np.asarray(Pxxm)\n",
    "        Pxx = pd.DataFrame(data=Pxxm,index=Pxx.index[bins],columns=Pxx.columns)\n",
    "        return Pxx, labels\n",
    "    \n",
    "    else:\n",
    "        return Pxx.iloc[bins,:], labels\n",
    "\n",
    "\n",
    "#extract features from both sensors (accel and gyro) for current clips and trials\n",
    "#input: dictionary of clips from each subject\n",
    "#output: feature matrix from all clips from given subject and scores for each clip\n",
    "def feature_extraction(clip_data):\n",
    "    \n",
    "    features_list = ['EX','EY','EZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ']\n",
    "    \n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "#                 print(rawdata.head(3))\n",
    "                \n",
    "                #extract features on current clip\n",
    "                \n",
    "                #Energy of signal on each axis\n",
    "                E = np.asarray(np.sum(rawdata**2,axis=0))\n",
    "                \n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "            \n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "                \n",
    "                #Power of FFT between 1-10 Hz\n",
    "                Pxx,fft_labels = powerspectra(rawdata,1,10) #dataframe with power spectra for each axis\n",
    "                xfft = np.asarray([Pxx.iloc[:,0].values, Pxx.iloc[:,1].values, Pxx.iloc[:,2].values])\n",
    "                xfft = np.reshape(xfft,(1,xfft.size)) #row vector\n",
    "                xfft = xfft.reshape(-1)\n",
    "\n",
    "                #Assemble features in array\n",
    "                x = np.concatenate((E,r,mean,var,sk,kurt,xfft))\n",
    "#                 x = np.asarray([E,r,mean,var,sk,kurt,xfft]) #features for 1 clip\n",
    "#                 x = np.reshape(x,(1,x.size)) #row vector\n",
    "                features.append(x)\n",
    "                    \n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "            clip_data[trial][sensor]['features'] = pd.DataFrame(data=F,columns=features_list+fft_labels,dtype='float32')  \n",
    "        \n",
    "#     return clip_data #not necessary\n",
    "\n",
    "\n",
    "#highpass filter data to remove gravity (offset - limb orientation) from accelerometer data from each visit (trial)\n",
    "#input: Activity dictionary, cutoff freq [Hz], task and sensor location to filter\n",
    "def HPfilter(act_dict,task,loc,cutoff=0.75):\n",
    "\n",
    "    sensor = 'accel'\n",
    "    for trial in act_dict[task].keys():\n",
    "        rawdata = act_dict[task][trial][loc][sensor]\n",
    "        idx = rawdata.index\n",
    "        idx = idx-idx[0]\n",
    "        rawdata.index = idx\n",
    "        x = rawdata.values \n",
    "        Fs = np.mean(1/(np.diff(rawdata.index)/1000)) #sampling rate    \n",
    "        #filter design\n",
    "        cutoff_norm = cutoff/(0.5*Fs)\n",
    "        b,a = butter(4,cutoff_norm,btype='highpass',analog=False)\n",
    "        #filter data\n",
    "        xfilt = filtfilt(b,a,x,axis=0)\n",
    "        rawdatafilt = pd.DataFrame(data=xfilt,index=rawdata.index,columns=rawdata.columns)\n",
    "        act_dict[task][trial][loc][sensor] = rawdatafilt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Error Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_errors(participant, timestamps):\n",
    "#input: 4 digit participant ID\n",
    "#Output: ErrorList - A list of the errors needed to be fixed for the participant\n",
    "#        timestamps - The dataFrame with the errors corrected for the participant\n",
    "#        errordf - The dataFrame containing the remaining errors less the ones just fixed\n",
    "\n",
    "    participant = int(participant) #convert to int, input SubID is a str\n",
    "\n",
    "    errordf = pd.read_excel(os.path.join(folder_path, 'PD_errorWorkbook.xlsx'))\n",
    "    errPar = errordf[errordf['Participant'] == participant]\n",
    "    errorActivity = (errPar['Activity'])\n",
    "    error = errPar['Error']\n",
    "    cycle = errPar['Cycle']\n",
    "    day = errPar['Day']\n",
    "    time = errPar['Time Adjusted (sec)']\n",
    "    desc = errPar['Type']\n",
    "    errorAndActivity = errPar[['Error','Activity']]\n",
    "    \n",
    "    for a in range(0,len(error)):\n",
    "        errAct = (errorActivity.iloc[a])\n",
    "        errType = (error.iloc[a])\n",
    "        errCycle = (cycle.iloc[a])\n",
    "        errTime = (time.iloc[a])\n",
    "        errDesc = (desc.iloc[a])\n",
    "        errDay = (day.iloc[a])\n",
    "        # print('\\n\\n\\n',type(errDay))\n",
    "        if errDay == 'Day 2':\n",
    "            if 'MDS' in errAct:\n",
    "                errCycle = errCycle + 2\n",
    "            elif 'Motor' in errAct:\n",
    "                errCycle = errCycle + 5\n",
    "\n",
    "        if errType == 'Merge':\n",
    "            timestamps = fix_merge(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Late':\n",
    "            timestamps = fix_late(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Early':\n",
    "            timestamps = fix_early(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Duplicate':\n",
    "            timestamps = fix_duplicate(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay,participant)\n",
    "\n",
    "        elif errType == 'Split':\n",
    "            timestamps = fix_split(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "            \n",
    "        elif errType == 'Absent':\n",
    "            timestamps = fix_absent(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "            \n",
    "    for a in range(0,len(error)):\n",
    "        errAct = (errorActivity.iloc[a])\n",
    "        errType = (error.iloc[a])\n",
    "        errCycle = (cycle.iloc[a])\n",
    "        errTime = (time.iloc[a])\n",
    "        errDesc = (desc.iloc[a])\n",
    "        errDay = (day.iloc[a])\n",
    "        \n",
    "        if errType == 'Absent':\n",
    "            timestamps = fix_absent(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "            \n",
    "    tempappend = errordf.loc[errPar.index.values]\n",
    "    #fixdf = fixdf.append(tempappend)\n",
    "    errordf = errordf.drop(errPar.index.values)\n",
    "    \n",
    "    print('Subject ' + str(participant) + ' had ' + str(len(error)) + ' errors fixed.')\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def fix_late(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "# subtracts time from the beginning or ending timestamp of the designated activity\n",
    "\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        startRow = timestamps.iloc[i]\n",
    "        if timestamps.index[i] == errAct and startRow['Cycle'] == errCycle:\n",
    "            errorLocation = i\n",
    "      \n",
    "            if errType == 'End':\n",
    "                startTime = startRow['Stop Timestamp (ms)']\n",
    "                startTime = startTime - (errTime*1000)\n",
    "                ii = timestamps.columns.get_loc('Stop Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True)    \n",
    "        \n",
    "            else:\n",
    "                startTime = startRow['Start Timestamp (ms)']\n",
    "                # print(startRow)\n",
    "                # print(startTime)\n",
    "                startTime = startTime - (errTime*1000)\n",
    "                # print(errTime)\n",
    "                # print(startTime)\n",
    "                ii = timestamps.columns.get_loc('Start Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True)\n",
    "     \n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def fix_early(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "# adds time to the beginning or ending timestamp of the designated activity\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        startRow = timestamps.iloc[i]\n",
    "        \n",
    "        if timestamps.index[i] == errAct and startRow['Cycle'] == errCycle:\n",
    "            errorLocation = i\n",
    "            \n",
    "            if errType == 'End':\n",
    "                startTime = startRow[1]\n",
    "                startTime = startTime + (errTime*1000)\n",
    "                ii = timestamps.columns.get_loc('Stop Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True)    \n",
    "\n",
    "            else:\n",
    "                startTime = startRow[0]\n",
    "                startTime = startTime + (errTime*1000)\n",
    "                ii = timestamps.columns.get_loc('Start Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True) \n",
    "\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def fix_merge(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "    \n",
    "    for i in range(0,len(timestamps)-2):\n",
    "        nextRow = timestamps.iloc[i+1]\n",
    "        startRow = timestamps.iloc[i]\n",
    "\n",
    "        if timestamps.index[i] == errAct and startRow['Cycle'] == errCycle:\n",
    "            timeEnd = nextRow['Stop Timestamp (ms)']\n",
    "            timestamps = timestamps.set_value(timestamps.index[i],'Stop Timestamp (ms)',timeEnd)\n",
    "            timestamps = pd.concat([timestamps.iloc[:(i+1)],timestamps.iloc[(i+2):]])\n",
    "\n",
    "    timestamps.reset_index(inplace=True)\n",
    "    timestamps['Cycle'] = timestamps.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    timestamps.set_index('EventType',inplace=True)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "                \n",
    "\n",
    "def fix_split(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        row = timestamps.iloc[i]\n",
    "        \n",
    "        if timestamps.index[i] == errAct and row['Cycle'] == errCycle:\n",
    "            timeStart1 = row['Start Timestamp (ms)']\n",
    "            timeEnd2 = row['Stop Timestamp (ms)']\n",
    "            timeChange = errTime\n",
    "            timeEnd1 = timeStart1 + timeChange\n",
    "            timeStart2 = timeEnd1\n",
    "            idx = complete.index(errAct)\n",
    "            ErrorActivity2 = complete[idx+1]\n",
    "            timestamps.set_value(timestamps.index[i],'Stop Timestamp (ms)',timeEnd1)\n",
    "            line = pd.DataFrame({\"Start Timestamp (ms)\":timeEnd1,\"Stop Timestamp (ms)\":timeEnd2,\"Cycle\":errCycle},index=[ErrorActivity2])\n",
    "            timestamps = pd.concat([timestamps.iloc[:(i+1)],line,timestamps.iloc[(i+1):]])\n",
    "            \n",
    "            timestamps.reset_index(inplace=True)\n",
    "            colnames = timestamps.columns.tolist()\n",
    "            colnames[colnames.index('index')] = 'EventType'\n",
    "            timestamps.columns = colnames\n",
    "            timestamps['Cycle'] = timestamps.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "            timestamps.set_index('EventType',inplace=True)\n",
    "           \n",
    "    return timestamps\n",
    "\n",
    "            \n",
    "\n",
    "def fix_duplicate(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay,participant):\n",
    "\n",
    "    for i in range(0,len(timestamps)-2):\n",
    "        row = timestamps.iloc[i]\n",
    "        if participant == 1052 and timestamps.index[i] == 'MDS-UPDRS #6: Arising from Chair':\n",
    "            if timestamps.index[i] == errAct and row['Cycle'] == errCycle+1:\n",
    "                timestamps = pd.concat([timestamps.iloc[:(i)],timestamps.iloc[(i+1):]])\n",
    "            \n",
    "        elif timestamps.index[i] == errAct and row['Cycle'] == errCycle:\n",
    "            timestamps = pd.concat([timestamps.iloc[:i],timestamps.iloc[(i+1):]])\n",
    "            \n",
    "    timestamps.reset_index(inplace=True)\n",
    "    timestamps['Cycle'] = timestamps.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    timestamps.set_index('EventType',inplace=True)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "def fix_absent(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "    \n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        row = timestamps.iloc[i]\n",
    "        \n",
    "        if timestamps.index[i] == errAct and row['Cycle'] == errCycle:\n",
    "\n",
    "            for j in range(i-1,len(timestamps)-1):\n",
    "                row = timestamps.iloc[j]\n",
    "                \n",
    "                if timestamps.index[j] == errAct:\n",
    "                    cyclenum = row['Cycle']\n",
    "                    newCycle = cyclenum + 1\n",
    "                    ii = timestamps.columns.get_loc('Cycle')\n",
    "                    timestamps.set_value(j,ii,newCycle,takeable=True)  \n",
    "    \n",
    "    return timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errordf = pd.read_excel(r'X:\\CIS-PD Study\\PD_errorWorkbook.xlsx')\n",
    "errordf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path + '/1020/anterior_thigh_left/d5la7wz0/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionaries from sensor data from all the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all subj data files in repository\n",
    "d = os.listdir(path)\n",
    "f = [filename[0:4] for filename in d if filename.startswith('1')] #need to update to skip existing files in /data\n",
    "print(f)\n",
    "#existing data dictionary files\n",
    "fd = os.listdir(dict_path)\n",
    "fd = [x[:4] for x in fd] #ignore FX at end\n",
    "print(list(set(f) - set(fd)))\n",
    "\n",
    "#errors temporarily resolved 10/10/2017, will update later\n",
    "# f.remove('1030')\n",
    "# f.remove('1032')\n",
    "# f.remove('1024')\n",
    "# f.remove('1052')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create data dict for remaining subjects\n",
    "data_all = []\n",
    "for SubID in list(set(f)-set(fd)):\n",
    "    print('Loading Subject ' + SubID + ' Data...')\n",
    "    act_dict, timestamps, testInfo = extract_data(SubID, os.path.join(path, SubID))\n",
    "    print('Extract data complete.')\n",
    "    #save dict to Pickle file\n",
    "    #filename = dict_path+'\\\\'+SubID + 'dict.pkl'\n",
    "    filename = os.path.join(dict_path, SubID + 'dict.pkl')\n",
    "    print(filename)\n",
    "    f = open(filename,'wb')\n",
    "    pickle.dump(act_dict,f)\n",
    "    f.close()\n",
    "    print(filename + ' ' + 'File Saved\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore features from individual subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 1020\n",
    "#f = open(dict_path+'/'+str(subj)+'dict.pkl','rb')\n",
    "f = open(os.path.join(dict_path, str(subj) + 'dict.pkl'), 'rb')\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choose task, trials(visits) and sensor location\n",
    "# task = 'Motor #8: Typing on a Computer'\n",
    "task = 'Motor #5: Alternating Hand Movements'\n",
    "\n",
    "loc = 'dorsal_hand_left'\n",
    "# loc = 'dorsal_hand_right'\n",
    "# loc = 'sacrum'\n",
    "# loc = 'flexor_digitorum_left'\n",
    "# loc = 'flexor_digitorum_right'\n",
    "sensor = 'accel'\n",
    "trial = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(act_dict[task].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = act_dict[task][trial][loc][sensor]\n",
    "idx = rawdata.index\n",
    "idx = idx-idx[0]\n",
    "rawdata.index = idx\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Filter raw accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'dorsal_hand_left'\n",
    "HPfilter(act_dict,task=task,loc=loc)\n",
    "act_dict[task][0][loc][sensor].plot(figsize=(8,4))\n",
    "ax = plt.ylim([-0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'dorsal_hand_left'\n",
    "HPfilter(act_dict,task=task,loc=loc)\n",
    "act_dict[task][1][loc][sensor].plot(figsize=(8,4))\n",
    "ax = plt.ylim([-0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'dorsal_hand_right'\n",
    "HPfilter(act_dict,task=task,loc=loc)\n",
    "act_dict[task][1][loc][sensor].plot(figsize=(8,4))\n",
    "ax = plt.ylim([-0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data = gen_clips(act_dict,task=task,clipsize=10000,location=loc,overlap=0,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data[0][sensor]['clip_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate clips data for left and right limb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataR = gen_clips(act_dict,task=task,clipsize=5000,location=loc,overlap=0,verbose=True,startTS=0,endTS=0.5)\n",
    "clip_dataL = gen_clips(act_dict,task=task,clipsize=5000,location=loc,overlap=0,verbose=True,startTS=0.5,endTS=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataL[1][sensor]['clip_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Let's start with the following:\n",
    "* Energy (total within segment)\n",
    "* Max\n",
    "* Min\n",
    "* Mean\n",
    "* Variance\n",
    "* Skewness\n",
    "* Kurtosis\n",
    "* Power spectra 0-10 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature calculation test\n",
    "# E = np.asarray(np.sum(rawdata**2,axis=0))\n",
    "# mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "# var = np.asarray(np.std(rawdata,axis=0))\n",
    "# sk = skew(rawdata)\n",
    "# kurt = kurtosis(rawdata)\n",
    "# min_xyz = np.min(rawdata,axis=0)\n",
    "# max_xyz = np.max(rawdata,axis=0)\n",
    "# r = np.asarray(max_xyz-min_xyz)\n",
    "        \n",
    "# xfft = np.asarray([Pxx.iloc[:,0].values,Pxx.iloc[:,1].values,Pxx.iloc[:,2].values])\n",
    "# xfft = np.reshape(xfft,(1,xfft.size)) #row vector\n",
    "# xfft = xfft.reshape(-1)\n",
    "\n",
    "# x = np.concatenate((E,r,mean,var,sk,kurt,xfft))\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot power spectra from one clip **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = 11 #number of bins for power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rawdata = clip_data[0][sensor]['data'][0] #trial-sensor-clip#\n",
    "Pxx,fft_labels = powerspectra(rawdata,0,10,nbins=nb,binavg=True) #dataframe with power spectra for each axis\n",
    "Pxx.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rawdata = clip_data[0][sensor]['data'][0] #trial-sensor-clip#\n",
    "Pxx,fft_labels = powerspectra(rawdata,0,10,nbins=nb,binavg=False) #dataframe with power spectra for each axis\n",
    "Pxx.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = clip_data[4][sensor]['data'][0] #trial-sensor-clip#\n",
    "Pxx,fft_labels = powerspectra(rawdata,0,10,nbins=nb,binavg=False) #dataframe with power spectra for each axis\n",
    "Pxx.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Power spectra with Welch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal features\n",
    "rawdata = clip_data[0][sensor]['data'][0] #trial-sensor-clip#\n",
    "fig = plt.figure()\n",
    "# fp, ax_arr = plt.subplots(3, sharex=True)\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_subplot(3,1,i+1)\n",
    "    x = rawdata.iloc[:,i]\n",
    "    n = len(x) #number of samples in clip\n",
    "    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "    f,Pxx_den = welch(x,Fs,nperseg=256)\n",
    "    plt.plot(f,Pxx_den)\n",
    "#     plt.semilogy(f,Pxx_den)\n",
    "    plt.xlim([0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extraction(clip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data[1]['accel']['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate features data with scores for given task - cycle through all subjects\n",
    "for s in subjects:\n",
    "*    load score files\n",
    "*    load subject file\n",
    "*    choose task and sensor location\n",
    "*    extract clips\n",
    "*    compute features on each trial\n",
    "*    Aggregate subj code and score with feature matrix\n",
    "\n",
    "**Note: gyro data has to be added **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aggreagate features (input) and metadata(output) for classification stage\n",
    "#append is a flag on whether to create a new data matrix or aggregate new rows to existing one\n",
    "#startTS and endTS specificies fraction of raw sensors data to use to generate clips\n",
    "#side specificies which score side to use (recommended to use the same side of the loca)\n",
    "def DataAggregator(task,task_scores,locs,side,clipsize=5000,overlap=0.5,startTS=0,endTS=1,append=1):\n",
    "    \n",
    "\n",
    "    #load subject scores (std motor assessments)\n",
    "    mot_scores = pd.read_excel(os.path.join(scores_path, 'MotorTasks.xls'))\n",
    "    #remove words:(Qxx) and 'rating' from each column for readability\n",
    "    cols= mot_scores.columns\n",
    "    cols = cols[4:]\n",
    "    cols = cols.tolist()\n",
    "    colsnew = [x.split('(')[0] for x in cols]\n",
    "    colsnew = [x.strip() for x in colsnew] #remove whitspace\n",
    "    colsnew = [x.split('rating')[0] for x in colsnew]\n",
    "    colsnew = [x.strip() for x in colsnew]\n",
    "    c = dict(zip(cols,colsnew))\n",
    "    mot_scores = mot_scores.rename(index=str, columns=c)\n",
    "\n",
    "    #load subjects features data and assemble with scores/subj metadata\n",
    "    d = os.listdir(dict_path)\n",
    "    fnames = [filename for filename in d if filename.startswith('1')]\n",
    "    print(fnames)\n",
    "    print('Aggregating scores for %s side'%side)\n",
    "\n",
    "    Data = pd.DataFrame() #the table with all data\n",
    "    for subj_filename in fnames:\n",
    "\n",
    "        #extract current subject scores and metadata\n",
    "        subj = int(subj_filename[:4]) #subj code\n",
    "        #extract scores for corresponding task, subject and side\n",
    "        subj_score = mot_scores.loc[mot_scores['Subject']==subj,['Subject','Visit',\n",
    "                                    task_scores+ ' ' + 'bradykinesia ' + side + ' upper limb',\n",
    "                                    task_scores+ ' ' + 'tremor ' + side + ' upper limb',\n",
    "                                    task_scores+ ' ' + 'overall score']]\n",
    "        #rename cols\n",
    "        subj_score = subj_score.rename(index=str,\n",
    "                                       columns={subj_score.columns[2]:'Bradykinesia',subj_score.columns[3]:'Tremor', \n",
    "                                               subj_score.columns[4]:'overall'})\n",
    "        subj_score.index = range(len(subj_score))\n",
    "        if len(subj_score) < 1:\n",
    "            print('no scores data for subject %d -- skipping..'%subj)\n",
    "\n",
    "        #load subject sensor data Pickle file (dictionary)\n",
    "        else:\n",
    "            f = open(os.path.join(dict_path,subj_filename),'rb')    \n",
    "            act_dict = pickle.load(f)\n",
    "            f.close()\n",
    "            print('\\nLoaded Subj %s sensor data'%subj)\n",
    "\n",
    "            #loop through sensor locations\n",
    "            for loc in locs:\n",
    "\n",
    "                #high pass filter accelerometer data\n",
    "                HPfilter(act_dict,task,loc)\n",
    "\n",
    "                #generate clips and extract features\n",
    "                clip_data = gen_clips(act_dict,task,loc,clipsize,overlap,False,startTS,endTS)\n",
    "                feature_extraction(clip_data)\n",
    "\n",
    "                #aggreagate subject, scores and features data\n",
    "                n_visits = len(subj_score)    #of visits in Database\n",
    "                n_rec = len(clip_data.keys()) #of sensor recordings\n",
    "                print('n_visits in score file = %d, # recordings = %d, location: %s'%(n_visits,n_rec,loc))\n",
    "                N = n_visits\n",
    "\n",
    "                if n_visits != n_rec:\n",
    "                    print('# of recordings does not match # of visits! - matching first %d recordings'%(min([n_visits,n_rec])))\n",
    "                    N = min([n_visits,n_rec])\n",
    "\n",
    "                #aggregate data from each visit for current subject\n",
    "                for i in range(N):\n",
    "                    #features\n",
    "                    D = clip_data[i]['accel']['features']\n",
    "                    featcols = D.columns.tolist()\n",
    "                    #scores\n",
    "                    D['Bradykinesia'] = subj_score['Bradykinesia'][i]\n",
    "                    D['Tremor'] = subj_score['Tremor'][i]\n",
    "                    #metadata\n",
    "                    D['Visit'] = subj_score.Visit[i] \n",
    "                    D['Task'] = task\n",
    "                    D['Location'] = loc \n",
    "                    D['Subject'] = subj\n",
    "                    Data = pd.concat([Data,D]) #concatenate data from each visit\n",
    "\n",
    "    cols = ['Subject','Visit','Task','Location','Bradykinesia','Tremor']+ featcols\n",
    "    Data = Data[cols]    \n",
    "    print('\\nData matrix generated')\n",
    "    print(Data.shape)\n",
    "\n",
    "    #save data to feature matrix\n",
    "    if append:\n",
    "        Data.to_csv(os.path.join(features_path,'Data.csv'),mode='a+',header=False) #append to existing\n",
    "        print('Appending to existing Feature matrix ' + str(os.path.join(features_path,'Data.csv')))    \n",
    "            \n",
    "    else:\n",
    "        Data.to_csv(os.path.join(features_path,'Data.csv'))\n",
    "        print('Feature matrix saved in ' + str(os.path.join(features_path,'Data.csv')))\n",
    "    \n",
    "#     return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task = 'Motor #5: Alternating Hand Movements'\n",
    "task_scores = 'Alternating right hand movements' #name of task in scores sheet\n",
    "locs = ['dorsal_hand_right'] #sensor location\n",
    "side = 'right' #score side to use (use same as sensor side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DataAggregator(task,task_scores,locs,side,startTS=0,endTS=0.5,append=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "fig.add_subplot(121)\n",
    "sns.distplot(Data['Bradykinesia left'],kde=False)\n",
    "sns.distplot(Data['Tremor left'],kde=False)\n",
    "plt.legend(['Bradyk','Tremor'])\n",
    "fig.add_subplot(122)\n",
    "sns.distplot(Data['Bradykinesia right'],kde=False)\n",
    "sns.distplot(Data['Tremor right'],kde=False)\n",
    "plt.legend(['Bradyk','Tremor'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize target scores into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rows with symptoms (Dataset w both hands)\n",
    "indpL = ((Data['Bradykinesia left']>0)&(Data['Location']=='dorsal_hand_left'))\n",
    "indpR = ((Data['Bradykinesia right']>0)&(Data['Location']=='dorsal_hand_right'))\n",
    "indp = indpL|indpR\n",
    "indp = indp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rows with symptoms (Dataset w one hand)\n",
    "indp = (Data['Bradykinesia left']>0) # | (Data['Bradykinesia right']>0)\n",
    "indp = indp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indn = (Data['Bradykinesia left']==0) & (Data['Bradykinesia right']==0) & (Data['Tremor left']==0) &(Data['Tremor right']==0) #no symptom\n",
    "indn = indn.values #1 = no symptom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Rows w bradykinesia %d/%d, Rows w no symptoms %d/%d'%(sum(indp),len(indp),sum(indn),len(indn)))\n",
    "print('Rows w bradykinesia %d/%d, Rows w no bradykinesia %d/%d'%(sum(indp),len(indp),sum(~indp),len(indp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on features to visualize subjects with bradykinesia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load feature matrix\n",
    "Data = pd.read_csv(os.path.join(features_path,'Data.csv'))\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardize features\n",
    "X = Data.iloc[:,8:]\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "nC = 10\n",
    "pca = PCA(n_components=nC)\n",
    "Xpca = pca.fit(X_std).transform(X_std)\n",
    "# Percentage of variance explained for each components\n",
    "print('total explained variance ratio (first %d components): %.3f'%(nC, pca.explained_variance_ratio_[0:nC].sum()))\n",
    "print(pca.explained_variance_ratio_[:10])\n",
    "ax = plt.plot(range(1,nC+1),pca.explained_variance_ratio_,'.-')\n",
    "plt.xlabel('# of components')\n",
    "plt.ylabel('% of variance explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xpca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(Xpca[indn,0],Xpca[indn,1],cmap=plt.cm.Set1,edgecolors='k',alpha=0.8,label='No symptom')\n",
    "plt.scatter(Xpca[indp,0],Xpca[indp,1],cmap=plt.cm.Set1,edgecolors='k',alpha=0.4,label='Bradykinesia')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(Xpca[~indp,0],Xpca[~indp,1],cmap=plt.cm.Set1,edgecolors='k',alpha=0.8,label='No Bradykinesia')\n",
    "plt.scatter(Xpca[indp,0],Xpca[indp,1],cmap=plt.cm.Set1,edgecolors='k',alpha=0.4,label='Bradykinesia')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit an LDA to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = indp.astype(int) #label values\n",
    "X_std.shape\n",
    "lda = LDA(n_components=3)\n",
    "X_lda = lda.fit_transform(X_std,y)\n",
    "fig=plt.figure()\n",
    "plt.plot(X_lda[~indp],np.zeros((X_lda[~indp].shape[0],1)),'.')\n",
    "plt.plot(X_lda[indp],np.ones((X_lda[indp].shape[0],1)),'r.')\n",
    "plt.xlabel('LDA feature')\n",
    "plt.ylabel('Bradykinesia presence')\n",
    "# X_lda_df = pd.DataFrame(data=np.hstack((X_lda,indp)),columns=['lda_feature','bradykinesia'])\n",
    "# X_lda_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit some classifiers to data using LOSOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper fcns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, norm=True):\n",
    "\n",
    "    if norm:\n",
    "        totals = cmat.sum(axis=1)\n",
    "        totals = totals.T\n",
    "        totals = np.expand_dims(totals,axis=1)\n",
    "        totals = np.tile(totals,(1,2))\n",
    "        cmat_norm = cmat/totals\n",
    "        print(cmat_norm)\n",
    "        cm = cmat_norm\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Bradykinesia','No-symptom'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['Bradykinesia','No-symptom'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "    \n",
    "def LOSOCV(Data,X,y,groups,models):\n",
    "\n",
    "    subj = LeaveOneGroupOut() \n",
    "\n",
    "    #train multiple classifiers\n",
    "    for m in models:\n",
    "        Sens_all=[]; Spec_all=[]; acc_all=[] #sens and spec for current model and all subject\n",
    "        clf = m[0]; model_name = m[1]            \n",
    "        print('Training %s'%model_name)\n",
    "        \n",
    "        #LOSO CV for current classifier\n",
    "        for train_index, test_index in subj.split(X, y, groups):\n",
    "            Xtr, Xte = X[train_index], X[test_index]\n",
    "            ytr, yte = y[train_index], y[test_index]\n",
    "            clf.fit(Xtr,ytr)\n",
    "            ypred = clf.predict(Xte)\n",
    "            yscore = clf.predict_proba(Xte)\n",
    "            yscore = yscore[:,1]\n",
    "\n",
    "            #compute Sens and Spec for current subject and classifier\n",
    "            if np.sum(yte==1)>0:                    \n",
    "                Sens = sum((ypred==1)&( yte==1))/sum(yte==1)\n",
    "            else:\n",
    "                print('%s has no positive examples'%Data.Subject[test_index].unique())\n",
    "                Sens = np.nan\n",
    "            if np.sum(yte==0)>0:\n",
    "                Spec = sum((ypred==0) & (yte==0))/sum(yte==0)\n",
    "            else:\n",
    "                Spec = np.nan\n",
    "                print('%s has no negative examples'%Data.Subject[test_index].unique())\n",
    "            Sens_all.append(Sens); Spec_all.append(Spec)\n",
    "            #compute accuracy\n",
    "            acc = sum(ypred==yte)/len(yte)\n",
    "            acc_all.append(acc)\n",
    "        \n",
    "        #mean across all subjects\n",
    "        print(('%s, mean Sens = %.3f (+/- %0.3f)')%(model_name,np.nanmean(Sens_all),2*np.nanstd(Sens_all)/np.sqrt(len(Sens_all))))\n",
    "        print(('%s, mean Spec = %.3f (+/- %0.3f)')%(model_name,np.nanmean(Spec_all),2*np.nanstd(Spec_all)/np.sqrt(len(Spec_all))))\n",
    "        print(('%s, mean Acc = %.3f (+/- %0.3f)')%(model_name,np.nanmean(acc_all),2*np.nanstd(acc_all)/np.sqrt(len(acc_all))))\n",
    "        \n",
    "        \n",
    "\n",
    "def plot_roc(tpr_all,fpr,roc_auc,ax=None,plotname=None,col=None):\n",
    "    #plot mean ROC across subjects (need to add shaded conf interval)\n",
    "    tprmu = np.mean(np.asarray(tpr_all),axis=0)\n",
    "    tpr=np.asarray(tpr_all)\n",
    "    fpr=np.reshape(fpr,(1,-1))\n",
    "    tprmu=np.reshape(tprmu,(1,-1))\n",
    "    label=pd.Series(data = ['%s - AUC = %0.3f' % (plotname,roc_auc)]*len(fpr))\n",
    "    if plotname=='Threshold':\n",
    "        ls = '-'\n",
    "    else:\n",
    "        ls='-'\n",
    "    if ax == None:\n",
    "        ax = sns.tsplot(data=tpr,time=fpr,ci=95,condition=label,legend=True,color=col,lw=3,linestyle=ls)\n",
    "    else:\n",
    "        sns.tsplot(data=tpr,time=fpr,ci=95,condition=label, legend=True,ax=ax,color=col,lw=3,linestyle=ls)\n",
    "             \n",
    "    lw = 3\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([-0.05, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=16)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data.Subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LOSO CV\n",
    "groups = Data.Subject.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(penalty='l1',random_state=2),\n",
    "          RandomForestClassifier(n_estimators=100,random_state=2),\n",
    "          GradientBoostingClassifier(n_estimators=100,max_depth=2,random_state=3),\n",
    "          SVC(kernel='linear',C=1,cache_size=800,probability=True,random_state=3),\n",
    "         ]\n",
    "model_name = ['LR','Random Forest','Gradient Boosting','SVM']\n",
    "models = list(zip(models,model_name))\n",
    "# models_list = list(models) #to save it as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = indp.astype(int) #target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LOSOCV(Data,X_std,y,groups,models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC STUFF - To Clean/Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on individual subjects / features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load subject scores\n",
    "#path = '../Scores/'\n",
    "mot_scores = pd.read_excel(os.path.join(scores_path, 'MotorTasks.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mot_scores[mot_scores['Subject']==1016].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subj = 1016\n",
    "#extract scores for corresponding task \n",
    "task = 'Motor #8: Typing on a Computer'\n",
    "loc = 'dorsal_hand_right'\n",
    "\n",
    "subj=1016\n",
    "subj_score = mot_scores.loc[mot_scores['Subject']==subj,['Subject','Visit',\n",
    "#                 'Typing on a computer keyboard overall score (Q92)',\n",
    "#                 'Typing on a computer keyboard bradykinesia left upper limb rating (Q93)',\n",
    "#                 'Typing on a computer keyboard tremor left upper limb rating (Q97)',\n",
    "                'Typing on a computer keyboard bradykinesia right upper limb rating (Q94)',\n",
    "                'Typing on a computer keyboard tremor right upper limb rating (Q98)']]\n",
    "\n",
    "subj_score = subj_score.rename(index=str,columns={subj_score.columns[2]:'Bradykinesia right',subj_score.columns[3]:'Tremor right' })\n",
    "subj_score.index = range(len(subj_score))\n",
    "subj_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aggreagate subject, scores and features data\n",
    "Data = pd.DataFrame()\n",
    "D = pd.DataFrame()\n",
    "\n",
    "n_visits = len(subj_score)    # # of visits in Database\n",
    "n_rec = len(clip_data.keys()) # # of sensor recordings\n",
    "print('n_visits = %d, # recordings = %d'%(n_visits,n_rec))\n",
    "N = n_visits\n",
    "\n",
    "if n_visits != n_rec:\n",
    "    print('# of recordings does not match # of visits! - matching first %d recordings')%(min([n_visits,n_rec]))\n",
    "    N = min([n_visits,n_rec])\n",
    "\n",
    "for i in range(N):\n",
    "    #features\n",
    "    D = clip_data[i]['accel']['features']\n",
    "    featcols = D.columns.tolist()\n",
    "    #scores\n",
    "    D['Bradykinesia right'] = subj_score['Bradykinesia right'][i]\n",
    "    D['Tremor right'] = subj_score['Tremor right'][i]\n",
    "    #metadata\n",
    "    D['Visit'] = subj_score.Visit[i] \n",
    "    D['Task'] = task\n",
    "    D['Location'] = loc \n",
    "    Data = pd.concat([Data,D]) #concatenate data from each visit\n",
    "    \n",
    "Data['Subject'] = subj \n",
    "cols = ['Subject','Visit','Task','Location','Bradykinesia right','Tremor right']+ featcols\n",
    "Data = Data[cols]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_acc = clip_data[0]['accel']['features']\n",
    "Data_gyr = clip_data[0]['accel']['features']\n",
    "print(Data_acc.shape,Data_gyr.shape)\n",
    "Data = pd.concat([Data_acc,Data_gyr],axis=1)\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_dict['Motor #10: Drinking Water'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks= ['Motor #13: Sitting','Motor #6: Sit to Stand','Motor #2: Walking','Motor #8: Typing on a Computer','Motor #4: Finger to Nose']\n",
    "trials = [0,5]\n",
    "# locs = ['dorsal_hand_right','flexor_digitorum_right','sacrum','anterior_thigh_right']\n",
    "# sensor = ['accel','gyro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = product(tasks,trials)\n",
    "taskslist = list(p)\n",
    "for t in taskslist:\n",
    "    plot_data(act_dict,t[0],t[1],'anterior_thigh_right','accel')\n",
    "    plt.title(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test itertools\n",
    "from itertools import product\n",
    "t = ('T1','T2')\n",
    "l = (1,2)\n",
    "s = ('s1','s2')\n",
    "# print(list(product(t,l,s)))\n",
    "taskslist = list(product(t,l,s))\n",
    "for t in taskslist:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data(act_dict,'Motor #6: Sit to Stand',0,'sacrum','accel')\n",
    "plot_data(act_dict,'Motor #6: Sit to Stand',5,'sacrum','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'anterior_thigh_left','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'anterior_thigh_left','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'flexor_digitorum_right','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'flexor_digitorum_right','accel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data(act_dict,'Motor #6: Sit to Stand',0,'sacrum','gyro')\n",
    "plot_data(act_dict,'Motor #6: Sit to Stand',5,'sacrum','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'anterior_thigh_left','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'anterior_thigh_left','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'flexor_digitorum_right','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'flexor_digitorum_right','gyro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errordf = pd.read_excel(r'X:\\CIS-PD Study\\PD_errorWorkbook.xlsx')\n",
    "errPar = errordf[errordf['Participant'] == 1020]\n",
    "errorActivity = (errPar['Activity'])\n",
    "error = errPar['Error']\n",
    "cycle = errPar['Cycle']\n",
    "day = errPar['Day']\n",
    "time = errPar['Time Adjusted (sec)']\n",
    "desc = errPar['Type']\n",
    "errorAndActivity = errPar[['Error','Activity']]\n",
    "# Add dynamic path for participant\n",
    "#testPath = r'X:\\CIS-PD Study\\Subjects\\1038'\n",
    "#(timestamps,random) = process_annotations(testPath)\n",
    "#pd.options.display.max_rows = 999\n",
    "#timestamps = fix_errors(1038)\n",
    "#timestamps"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
