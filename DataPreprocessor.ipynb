{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data preprocessing \n",
    "### load raw files -> generate clips -> compute features-> aggregate into a matrix of features and scores\n",
    "** Pandas version required to load pickle files is 0.20.1 or greater **\n",
    "\n",
    "* Try classifying symptom presence from \"typing\", \"walking\" and \"finger-to-nose\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import pickle #to save files\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import butter, welch, filtfilt\n",
    "\n",
    "# from PreprocessFcns import gen_clips, powerspectra\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---Pandas version required to load pickle files is 0.20.1 or greater---\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Set path to folder containing Subject ID numbers\n",
    "# path = '/Volumes/RT&O/CIS-PD Study/Subjects/' #Mac\n",
    "path = r'W:\\CIS-PD Study\\Subjects' #Windows local path adai\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "#Path where dictionary subject data is stored\n",
    "# dict_path = 'W:\\CIS-PD Study\\Data_dict' #remote repo\n",
    "# dict_path = '../Data_dict' #local path\n",
    "dict_path = r'C:\\Users\\adai\\Documents\\Data_dict' #Windows local path adai\n",
    "\n",
    "scores_path = r'W:\\CIS-PD Study\\Scores' #remote repo\n",
    "#scores_path = '../Scores/' #local path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete = list(['Heart Rate Variability', 'MDS-UPDRS #1: Finger Tapping',\n",
    "           'MDS-UPDRS #2: Hand Movements', 'MDS-UPDRS #3: Pronation-Supination',\n",
    "           'MDS-UPDRS #4: Toe Tapping', 'MDS-UPDRS #5: Leg Agility',\n",
    "           'MDS-UPDRS #6: Arising from Chair', 'MDS-UPDRS #7: Gait',\n",
    "           'MDS-UPDRS #8: Postural Stability', 'MDS-UPDRS #9: Postural Hand Tremor',\n",
    "           'MDS-UPDRS #10: Kinetic Hand Tremor', 'MDS-UPDRS #11: Rest Tremor',\n",
    "           'Motor #1: Standing', 'Motor #2: Walking', 'Motor #3: Walking while Counting',\n",
    "           'Motor #4: Finger to Nose', 'Motor #5: Alternating Hand Movements',\n",
    "           'Motor #6: Sit to Stand', 'Motor #7: Drawing on Paper',\n",
    "           'Motor #8: Typing on a Computer', 'Motor #9: Nuts and Bolts',\n",
    "           'Motor #10: Drinking Water', 'Motor #11: Organizing Folder',\n",
    "           'Motor #12: Folding Towels', 'Motor #13: Sitting'])\n",
    "\n",
    "\n",
    "def process_annotations(path):\n",
    "#def process_annotations(path, SubID):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Processes raw annotations file to extract start / end timestamps and remove unnecessary data\n",
    "#\n",
    "# Inputs:  path - filepath of the subject folder containing annotations.csv\n",
    "#\n",
    "# Outputs: df - dataframe containing list of activities and their start / end timestamps\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    df = pd.read_csv(os.path.join(path, 'annotations.csv'))\n",
    "    del df['Timestamp (ms)']\n",
    "    del df['AnnotationId']\n",
    "    del df['AuthorId']\n",
    "    \n",
    "    testInfo = df[df.EventType == 'Testing Day'].dropna(how='any', axis=0)\n",
    "    testInfo['Start Timestamp (ms)'] = pd.to_datetime(testInfo['Start Timestamp (ms)'], unit='ms', utc=True).dt.tz_localize('UTC').dt.tz_convert('US/Central')\n",
    "    del testInfo['Stop Timestamp (ms)']\n",
    "    del testInfo['EventType']\n",
    "    del df['Value']\n",
    "    \n",
    "    testInfo = testInfo.rename(columns = {'Value':'Day', 'Start Timestamp (ms)':'Date'}).reset_index(drop=True)\n",
    "    testInfo['Date'] = testInfo['Date'].dt.date\n",
    "    \n",
    "    df = df[(df.EventType != 'Testing Day')]\n",
    "    \n",
    "    sorter = set(df.EventType.unique().flatten())\n",
    "    sorterIndex = dict(zip(sorter, range(len(sorter))))\n",
    "        \n",
    "    df['EventType_Rank'] = df['EventType'].map(sorterIndex)\n",
    "    df['Cycle'] = df.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    del df['EventType_Rank']\n",
    "    df[df['EventType'].str.contains('Heart')] = df[df['EventType'].str.contains('Heart')].replace(to_replace={'Cycle': {1: 'NaN', 2: 'NaN', 3: 'NaN', 4: 'NaN'}})\n",
    "    df = df.reset_index(drop=True).set_index('EventType')\n",
    "    \n",
    "    # return d1_df, d2_df, df\n",
    "    return df, testInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper fcns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_data(SubID, path):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# For a given subject, extracts and separates accelerometer, gyroscope, and EMG/ECG data into trials and sensor per activity\n",
    "#\n",
    "# Inputs: SubID - string of numbers corresponding to the subject ID\n",
    "#         path - system path to corresponding subject's raw data files\n",
    "#\n",
    "# Outputs: act_dict - dictionary of both MDS-UPDRS and Motor Assessment activities separated by trial, sensor location, and\n",
    "#                     accelerometer + gyroscope or accelerometer + EMG/ECG data. Every key within this dictionary is a dictionary\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    #timestamps = process_annotations(path, SubID)\n",
    "    timestamps, testInfo = process_annotations(path)\n",
    "    \n",
    "    # Creates list of sensor locations from folders within subject's raw data directory\n",
    "    locations = [locs for locs in os.listdir(path) if os.path.isdir(os.path.join(path, locs))]\n",
    "    \n",
    "    # Creates dictionary of empty dataframes to merge all accelerometer, gyroscope, and EMG/ECG data for each sensor\n",
    "    accel = {locs: pd.DataFrame() for locs in locations}\n",
    "    gyro = {locs: pd.DataFrame() for locs in locations}\n",
    "    elec = {locs: pd.DataFrame() for locs in locations}\n",
    "    \n",
    "    # Finds and merges all accelerometer, gyroscope, and EMG/ECG data for each sensor, retains datetime information\n",
    "    for root, dirs, files in os.walk(path, topdown=True):\n",
    "        for filenames in files:\n",
    "            if filenames.endswith('accel.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                accel[location] = accel[location].append(temp_df)\n",
    "\n",
    "            elif filenames.endswith('gyro.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                gyro[location] = gyro[location].append(temp_df)\n",
    "\n",
    "            elif filenames.endswith(('elec.csv', 'emg.csv', 'ecg.csv', 'ekg.csv')):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                elec[location] = elec[location].append(temp_df)\n",
    "                \n",
    "    # Complete dictionary of all activities\n",
    "    act_dict = {acts: pd.DataFrame() for acts in complete}\n",
    "\n",
    "    # Populate dictionary keys per activity with every iteration / trial\n",
    "    for activities in complete:\n",
    "        print(timestamps.loc[activities, 'Start Timestamp (ms)'])\n",
    "        print(timestamps.loc[activities, 'Stop Timestamp (ms)'])\n",
    "        \n",
    "        startTimestamp = timestamps.loc[activities, 'Start Timestamp (ms)'].values\n",
    "        endTimestamp = timestamps.loc[activities, 'Stop Timestamp (ms)'].values\n",
    "\n",
    "        # Create trial dictionary with each key containing all sensor data related with each activity's trial\n",
    "        trial_dict = {trials: pd.DataFrame() for trials in range(0, len(startTimestamp))}\n",
    "\n",
    "        # Populate trial directory keys\n",
    "        for trials in range(0, len(startTimestamp)):\n",
    "\n",
    "            startTime = startTimestamp[trials]\n",
    "            endTime = endTimestamp[trials]\n",
    "\n",
    "            # Create sensor location dictionary with each key corresponding to sensor locations\n",
    "            sensor_dict = {locs: pd.DataFrame() for locs in locations}\n",
    "\n",
    "            # Extract sensor data and populate sensor_dict with sensor data\n",
    "            for location in locations:\n",
    "\n",
    "                data = {'accel': pd.DataFrame(), 'gyro': pd.DataFrame(), 'elec': pd.DataFrame()}\n",
    "\n",
    "                if not accel[location].empty:\n",
    "                    accelData = accel[location]\n",
    "                    data['accel'] = accelData[(accelData.index >= startTime) & (accelData.index <= endTime)]\n",
    "\n",
    "                if not gyro[location].empty:\n",
    "                    gyroData = gyro[location]\n",
    "                    data['gyro'] = gyroData[(gyroData.index >= startTime) & (gyroData.index <= endTime)]\n",
    "\n",
    "                if not elec[location].empty:\n",
    "                    elecData = elec[location]\n",
    "                    data['elec'] = elecData[(elecData.index >= startTime) & (elecData.index <= endTime)]\n",
    "\n",
    "                sensor_dict[location] = data\n",
    "\n",
    "            trial_dict[trials] = sensor_dict\n",
    "\n",
    "        act_dict[activities] = trial_dict\n",
    "    \n",
    "    return act_dict, timestamps, testInfo\n",
    "\n",
    "\n",
    "                                \n",
    "def plot_data(acts, activity, trial, sensor, data, start=0, end=100000):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Plots requested data\n",
    "# \n",
    "# Inputs: acts - activity dataframe containing all sensor data from one subject\n",
    "#         activity - desired activity to plot\n",
    "#         trial - desired trial number to plot\n",
    "#         sensor - desired sensor (serial number or name of location) to analyze\n",
    "#         data - desired type of data to analyze (accel, gyro, EMG/ECG)\n",
    "#         start - starting index, default starts at first point\n",
    "#         end - ending index, default is 500th data point\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    toPlot = pd.DataFrame(acts[activity][trial]).loc[data, sensor][start:end].plot(figsize=(6,4))\n",
    "    \n",
    "\n",
    "#extract clips for accelerometer and gyro data\n",
    "def gen_clips(act_dict,task,location,clipsize=5000,overlap=0,verbose=False):\n",
    "    \n",
    "    clip_data = {} #the dictionary with clips\n",
    "    #params\n",
    "    len_tol = 0.8   #% of the intended clipsize below which clip is not used\n",
    "\n",
    "    for trial in act_dict[task].keys():\n",
    "        clip_data[trial] = {}            \n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            #create clips data\n",
    "            deltat = np.median(np.diff(rawdata.index))\n",
    "            idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "            clips = []\n",
    "            for i in idx:\n",
    "                c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                if len(c) > 0.8*int(clipsize/deltat): #discard clips whose length is less than len_tol% of the window size\n",
    "                    clips.append(c)\n",
    "            clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] #store the length of each clip\n",
    "            #assemble in dict\n",
    "            clip_data[trial][s] = {'data':clips, 'clip_len':clip_len}\n",
    "\n",
    "    return clip_data\n",
    "\n",
    "\n",
    "#returns power spectra of the signal over each channel between min and max freq at given resolution (nbins)\n",
    "#returns the labels for each bin\n",
    "#if binavg is True it averages the PSD within bins to reduce PSD noise\n",
    "def powerspectra(x,fm,fM,nbins=10,relative=False,binavg=True):\n",
    "    \n",
    "    #feature labels\n",
    "    labels=[]\n",
    "    s = np.linspace(fm,fM,nbins)\n",
    "    lax = ['X','Y','Z']\n",
    "    for l in lax:\n",
    "        for i in s:\n",
    "            labels.append('fft'+l+str(int(i)))\n",
    "            \n",
    "    #signal features\n",
    "    n = len(x) #number of samples in clip\n",
    "    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "    timestep = 1/Fs \n",
    "    freq = np.fft.fftfreq(n,d=timestep) #frequency bins\n",
    "\n",
    "    #run FFT on each channel \n",
    "    Xf = x.apply(np.fft.fft)\n",
    "    Xf.index = np.round(freq,decimals=1) #reindex w frequency bin\n",
    "    Pxx = Xf.apply(np.abs)\n",
    "    Pxx = Pxx**2 #power spectra\n",
    "    if relative:\n",
    "        Pxx = Pxx/np.sum(Pxx,axis=0) #power relative to total\n",
    "    \n",
    "    #power spectra between fm-fM Hz\n",
    "    bin1 = int(timestep*n*fm)\n",
    "    bin2 = int(timestep*n*fM)\n",
    "    bins = np.linspace(bin1,bin2,nbins,dtype=int)\n",
    "#     print(bins/(round(timestep*n)))\n",
    "\n",
    "    #average power spectra within bins\n",
    "    if binavg:\n",
    "        deltab = int(0.5*np.diff(bins)[0]) #half the size of a bin (in samples)\n",
    "        Pxxm = []\n",
    "        for i in bins:\n",
    "            start = int(max(i-deltab,bins[0]))\n",
    "            end = int(min(i+deltab,bins[-1]))\n",
    "            Pxxm.append(np.mean(Pxx.iloc[start:end,:].values,axis=0))            \n",
    "        Pxxm = np.asarray(Pxxm)\n",
    "        Pxx = pd.DataFrame(data=Pxxm,index=Pxx.index[bins],columns=Pxx.columns)\n",
    "        return Pxx, labels\n",
    "    \n",
    "    else:\n",
    "        return Pxx.iloc[bins,:], labels\n",
    "\n",
    "\n",
    "#extract features from both sensors (accel and gyro) for current clips and trials\n",
    "#input: dictionary of clips from each subject\n",
    "#output: feature matrix from all clips from given subject and scores for each clip\n",
    "def feature_extraction(clip_data):\n",
    "    \n",
    "    features_list = ['EX','EY','EZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ']\n",
    "    \n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "#                 print(rawdata.head(3))\n",
    "                \n",
    "                #extract features on current clip\n",
    "                \n",
    "                #Energy of signal on each axis\n",
    "                E = np.asarray(np.sum(rawdata**2,axis=0))\n",
    "                \n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "            \n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "                \n",
    "                #Power of FFT between 1-10 Hz\n",
    "                Pxx,fft_labels = powerspectra(rawdata,1,10) #dataframe with power spectra for each axis\n",
    "                xfft = np.asarray([Pxx.iloc[:,0].values, Pxx.iloc[:,1].values, Pxx.iloc[:,2].values])\n",
    "                xfft = np.reshape(xfft,(1,xfft.size)) #row vector\n",
    "                xfft = xfft.reshape(-1)\n",
    "\n",
    "                #Assemble features in array\n",
    "                x = np.concatenate((E,r,mean,var,sk,kurt,xfft))\n",
    "#                 x = np.asarray([E,r,mean,var,sk,kurt,xfft]) #features for 1 clip\n",
    "#                 x = np.reshape(x,(1,x.size)) #row vector\n",
    "                features.append(x)\n",
    "                    \n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "            F = F.squeeze()            \n",
    "            clip_data[trial][sensor]['features'] = pd.DataFrame(data=F,columns=features_list+fft_labels,dtype='float32')  \n",
    "        \n",
    "#     return clip_data #not necessary\n",
    "\n",
    "\n",
    "#highpass filter data to remove gravity (offset - limb orientation) from accelerometer data from each visit (trial)\n",
    "#input: Activity dictionary, cutoff freq [Hz], task and sensor location to filter\n",
    "def HPfilter(act_dict,task,loc,cutoff=0.75):\n",
    "\n",
    "    sensor = 'accel'\n",
    "    for trial in act_dict[task].keys():\n",
    "        rawdata = act_dict[task][trial][loc][sensor]\n",
    "        idx = rawdata.index\n",
    "        idx = idx-idx[0]\n",
    "        rawdata.index = idx\n",
    "        x = rawdata.values \n",
    "        Fs = np.mean(1/(np.diff(rawdata.index)/1000)) #sampling rate    \n",
    "        #filter design\n",
    "        cutoff_norm = cutoff/(0.5*Fs)\n",
    "        b,a = butter(4,cutoff_norm,btype='highpass',analog=False)\n",
    "        #filter data\n",
    "        xfilt = filtfilt(b,a,x,axis=0)\n",
    "        rawdatafilt = pd.DataFrame(data=xfilt,index=rawdata.index,columns=rawdata.columns)\n",
    "        act_dict[task][trial][loc][sensor] = rawdatafilt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Error Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_errors(participant):\n",
    "#input: 4 digit participant ID\n",
    "#Output: ErrorList - A list of the errors needed to be fixed for the participant\n",
    "#        timestamps - The dataFrame with the errors corrected for the participant\n",
    "#        errordf - The dataFrame containing the remaining errors less the ones just fixed\n",
    "\n",
    "   # errordf = pd.read_excel(r'C:\\Users\\Alex\\OneDrive\\SRALAB\\PD\\PD_errorWorkbook.xlsx')\n",
    "    errordf = pd.read_excel(r'X:\\CIS-PD Study\\PD_errorWorkbook.xlsx')\n",
    "    errPar = errordf[errordf['Participant'] == participant]\n",
    "   # fixdf = pd.read_excel(r'C:\\Users\\Alex\\OneDrive\\SRALAB\\PD\\PD_fixedErrors.xlsx')\n",
    "    #fixdf = pd.read_excel(r'C:\\Users\\aboe.SMPP\\OneDrive\\SRALAB\\PD\\PD_fixedErrors.xlsx')\n",
    "    errorActivity = (errPar['Activity'])\n",
    "    error = errPar['Error']\n",
    "    cycle = errPar['Cycle']\n",
    "    day = errPar['Day']\n",
    "    time = errPar['Time Adjusted (sec)']\n",
    "    desc = errPar['Type']\n",
    "    errorAndActivity = errPar[['Error','Activity']]\n",
    "    path = r'X:\\CIS-PD Study\\Subjects\\1016'\n",
    "    timestamps = process_annotations(path)\n",
    "    \n",
    "    print('There are',str(len(error)),'errors to be fixed:\\n')\n",
    "    print(errorAndActivity)\n",
    "    \n",
    "    for a in range(0,len(error)):\n",
    "        errAct = (errorActivity.iloc[a])\n",
    "        errType = (error.iloc[a])\n",
    "        errCycle = (cycle.iloc[a])\n",
    "        errTime = (time.iloc[a])\n",
    "        errDesc = (desc.iloc[a])\n",
    "        errDay = (day.iloc[a])\n",
    "\n",
    "        if errType == 'Merge':\n",
    "            timestamps = fix_merge(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Late':\n",
    "            timestamps = fix_late(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Early':\n",
    "            timestamps = fix_early(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Duplicate':\n",
    "            timestamps = fix_duplicate(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "\n",
    "        elif errType == 'Split':\n",
    "            timestamps = fix_split(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "            \n",
    "        elif errType == 'Absent':\n",
    "            ;\n",
    "            \n",
    "    for a in range(0,len(error)):\n",
    "        errAct = (errorActivity.iloc[a])\n",
    "        errType = (error.iloc[a])\n",
    "        errCycle = (cycle.iloc[a])\n",
    "        errTime = (time.iloc[a])\n",
    "        errDesc = (desc.iloc[a])\n",
    "        errDay = (day.iloc[a])\n",
    "        \n",
    "        if errType == 'Absent':\n",
    "            timestamps = fix_absent(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay)\n",
    "            \n",
    "    tempappend = errordf.loc[errPar.index.values]\n",
    "    #fixdf = fixdf.append(tempappend)\n",
    "    errordf = errordf.drop(errPar.index.values)\n",
    "    \n",
    "   # timestamps.to_excel(r'C:\\Users\\Alex\\OneDrive\\SRALAB\\PD\\timestamps_corrected.xlsx')\n",
    "    #timestamps.to_excel(r'C:\\Users\\aboe.SMPP\\OneDrive\\SRALAB\\PD\\timestamps_corrected.xlsx')\n",
    "    #errordf.to_excel(r'C:\\Users\\Alex\\OneDrive\\SRALAB\\PD\\PD_errorWorkbook.xlsx')\n",
    "   # fixdf.to_excel(r'C:\\Users\\Alex\\OneDrive\\SRALAB\\PD\\PD_fixedErrors.xlsx')\n",
    "    #fixdf.to_excel(r'C:\\Users\\aboe.SMPP\\OneDrive\\SRALAB\\PD\\PD_fixedErrors.xlsx')\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def fix_late(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "# subtracts time from the beginning or ending timestamp of the designated activity\n",
    "\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        startRow = timestamps.iloc[i]\n",
    "        if timestamps.index[i] == errAct and startRow['Cycle'] == errCycle:\n",
    "            errorLocation = i\n",
    "      \n",
    "            if errType == 'End':\n",
    "                startTime = startRow['Stop Timestamp (ms)']\n",
    "                startTime = startTime - (errTime*1000)\n",
    "                ii = timestamps.columns.get_loc('Stop Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True)    \n",
    "        \n",
    "            else:\n",
    "                startTime = startRow['Start Timestamp (ms)']\n",
    "                print(startRow)\n",
    "                print(startTime)\n",
    "                startTime = startTime - (errTime*1000)\n",
    "                print(errTime)\n",
    "                print(startTime)\n",
    "                ii = timestamps.columns.get_loc('Start Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True)\n",
    "     \n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def fix_early(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "# adds time to the beginning or ending timestamp of the designated activity\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        startRow = timestamps.iloc[i]\n",
    "        \n",
    "        if timestamps.index[i] == errAct and startRow['Cycle'] == errCycle:\n",
    "            errorLocation = i\n",
    "            \n",
    "            if errType == 'End':\n",
    "                startTime = startRow[1]\n",
    "                startTime = startTime + (errTime*1000)\n",
    "                ii = timestamps.columns.get_loc('Stop Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True)    \n",
    "\n",
    "            else:\n",
    "                startTime = startRow[0]\n",
    "                startTime = startTime + (errTime*1000)\n",
    "                ii = timestamps.columns.get_loc('Start Timestamp (ms)')\n",
    "                timestamps.set_value(i,ii,startTime,takeable=True) \n",
    "\n",
    "    return timestamps\n",
    "\n",
    "\n",
    "def fix_merge(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "    \n",
    "    for i in range(0,len(timestamps)-2):\n",
    "        nextRow = timestamps.iloc[i+1]\n",
    "        startRow = timestamps.iloc[i]\n",
    "\n",
    "        if timestamps.index[i] == errAct and startRow['Cycle'] == errCycle:\n",
    "            timeEnd = nextRow['Stop Timestamp (ms)']\n",
    "            timestamps = timestamps.set_value(timestamps.index[i],'Stop Timestamp (ms)',timeEnd)\n",
    "            timestamps = pd.concat([timestamps.iloc[:i],timestamps.iloc[(i+1):]])\n",
    "\n",
    "    timestamps.reset_index(inplace=True)\n",
    "    timestamps['Cycle'] = timestamps.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    timestamps.set_index('EventType',inplace=True)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "                \n",
    "\n",
    "def fix_split(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        row = timestamps.iloc[i]\n",
    "        \n",
    "        if timestamps.index[i] == errAct and row['Cycle'] == errCycle:\n",
    "            timeStart1 = row['Start Timestamp (ms)']\n",
    "            timeEnd2 = row['Stop Timestamp (ms)']\n",
    "            timeChange = errTime\n",
    "            timeEnd1 = timeStart1 + timeChange\n",
    "            timeStart2 = timeEnd1\n",
    "            idx = complete.index(errAct)\n",
    "            ErrorActivity2 = complete[idx+1]\n",
    "            timestamps.set_value(timestamps.index[i],'Stop Timestamp (ms)',timeEnd1)\n",
    "            line = pd.DataFrame({\"Start Timestamp (ms)\":timeEnd1,\"Stop Timestamp (ms)\":timeEnd2,\"Cycle\":errCycle},index=[ErrorActivity2])\n",
    "            timestamps = pd.concat([timestamps.iloc[:(i+1)],line,timestamps.iloc[(i+1):]])\n",
    "            \n",
    "            timestamps.reset_index(inplace=True)\n",
    "            colnames = timestamps.columns.tolist()\n",
    "            colnames[colnames.index('index')] = 'EventType'\n",
    "            timestamps.columns = colnames\n",
    "            timestamps['Cycle'] = timestamps.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "            timestamps.set_index('EventType',inplace=True)\n",
    "           \n",
    "    return timestamps\n",
    "\n",
    "            \n",
    "\n",
    "def fix_duplicate(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "\n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        row = timestamps.iloc[i]\n",
    "\n",
    "        if timestamps.index[i] == errAct and row['Cycle'] == errCycle:\n",
    "            timestamps = pd.concat([timestamps.iloc[:i],timestamps.iloc[(i+1):]])\n",
    "            \n",
    "    timestamps.reset_index(inplace=True)\n",
    "    timestamps['Cycle'] = timestamps.groupby('EventType')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    timestamps.set_index('EventType',inplace=True)\n",
    "\n",
    "    return timestamps\n",
    "\n",
    "def fix_absent(timestamps,errType,errAct,errCycle,errTime,errDesc,errDay):\n",
    "    \n",
    "    for i in range(0,len(timestamps)-1):\n",
    "        row = timestamps.iloc[i]\n",
    "        \n",
    "        if timestamps.index[i] == errAct and row['Cycle'] == errCycle:\n",
    "\n",
    "            for j in range(i-1,len(timestamps)-1):\n",
    "                row = timestamps.iloc[j]\n",
    "                \n",
    "                if timestamps.index[j] == errAct:\n",
    "                    cyclenum = row['Cycle']\n",
    "                    newCycle = cyclenum + 1\n",
    "                    ii = timestamps.columns.get_loc('Cycle')\n",
    "                    timestamps.set_value(j,ii,newCycle,takeable=True)  \n",
    "    \n",
    "    return timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-08-03T17-44-51-148Z']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path+'/1020/anterior_thigh_left/d5la7wz0/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionaries from sensor data from all the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1004dict.pkl',\n",
       " '1016dict.pkl',\n",
       " '1018dict.pkl',\n",
       " '1019dict.pkl',\n",
       " '1020dict.pkl',\n",
       " '1029dict.pkl',\n",
       " '1044dict.pkl',\n",
       " '1046dict.pkl',\n",
       " '1049dict.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1004', '1016', '1018', '1019', '1020', '1024', '1029', '1030', '1032', '1044', '1046', '1049', '1052']\n",
      "['1052', '1024', '1032', '1030']\n"
     ]
    }
   ],
   "source": [
    "#all subj data files in repository\n",
    "d = os.listdir(path)\n",
    "f = [filename[0:4] for filename in d if filename.startswith('1')] #need to update to skip existing files in /data\n",
    "print(f)\n",
    "#existing data dictionary files\n",
    "fd = os.listdir(dict_path)\n",
    "fd = [x[:4] for x in fd] #ignore FX at end\n",
    "print(list(set(f) - set(fd)))\n",
    "\n",
    "#remove subject that failed - need to check w Andrew\n",
    "#f.remove('1030')\n",
    "#f.remove('1032')\n",
    "#f.remove('1024')\n",
    "f.remove('1052')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "#create data dict for remaining subjects\n",
    "data_all = []\n",
    "for SubID in list(set(f)-set(fd)):\n",
    "    print(SubID)\n",
    "    act_dict, timestamps, testInfo = extract_data(SubID, os.path.join(path, SubID))\n",
    "    print('Extract data complete')\n",
    "    #save dict to Pickle file\n",
    "    #filename = dict_path+'\\\\'+SubID + 'dict.pkl'\n",
    "    filename = os.path.join(dict_path, SubID + 'dict.pkl')\n",
    "    print(filename)\n",
    "    f = open(filename,'wb')\n",
    "    pickle.dump(act_dict,f)\n",
    "    f.close()\n",
    "    print(filename + ' ' + 'File Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save from individual subject (no longer needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #for an individual subject\n",
    "# SubID = '1020'\n",
    "# act_dict, timestamps, testInfo = extract_data(SubID, os.path.join(path, SubID))\n",
    "# #save dict to Pickle file\n",
    "# f = open('1020dict.pk1','wb')\n",
    "# pickle.dump(act_dict,f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore features from individual subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 1004\n",
    "#f = open(dict_path+'/'+str(subj)+'dict.pkl','rb')\n",
    "f = open(os.path.join(dict_path, str(subj) + 'dict.pkl'), 'rb')\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choose task, trials(visits) and sensor location\n",
    "task = 'Motor #8: Typing on a Computer'\n",
    "loc = 'dorsal_hand_left'\n",
    "# loc = 'dorsal_hand_right'\n",
    "# loc = 'sacrum'\n",
    "# loc = 'flexor_digitorum_left'\n",
    "# loc = 'flexor_digitorum_right'\n",
    "sensor = 'accel'\n",
    "trial = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(act_dict[task].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = act_dict[task][trial][loc][sensor]\n",
    "idx = rawdata.index\n",
    "idx = idx-idx[0]\n",
    "rawdata.index = idx\n",
    "rawdata.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Filter raw accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HPfilter(act_dict,task=task,loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict[task][0][loc][sensor].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data = gen_clips(act_dict,task=task,clipsize=10000,location=loc,overlap=0,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data[0][sensor]['clip_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Let's start with the following:\n",
    "* Energy (total within segment)\n",
    "* Max\n",
    "* Min\n",
    "* Mean\n",
    "* Variance\n",
    "* Skewness\n",
    "* Kurtosis\n",
    "* Power spectra 0-10 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature calculation test\n",
    "# E = np.asarray(np.sum(rawdata**2,axis=0))\n",
    "# mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "# var = np.asarray(np.std(rawdata,axis=0))\n",
    "# sk = skew(rawdata)\n",
    "# kurt = kurtosis(rawdata)\n",
    "# min_xyz = np.min(rawdata,axis=0)\n",
    "# max_xyz = np.max(rawdata,axis=0)\n",
    "# r = np.asarray(max_xyz-min_xyz)\n",
    "        \n",
    "# xfft = np.asarray([Pxx.iloc[:,0].values,Pxx.iloc[:,1].values,Pxx.iloc[:,2].values])\n",
    "# xfft = np.reshape(xfft,(1,xfft.size)) #row vector\n",
    "# xfft = xfft.reshape(-1)\n",
    "\n",
    "# x = np.concatenate((E,r,mean,var,sk,kurt,xfft))\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot power spectra from one clip **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = 11 #number of bins for power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rawdata = clip_data[0][sensor]['data'][0] #trial-sensor-clip#\n",
    "Pxx,fft_labels = powerspectra(rawdata,0,10,nbins=nb,binavg=True) #dataframe with power spectra for each axis\n",
    "Pxx.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rawdata = clip_data[0][sensor]['data'][0] #trial-sensor-clip#\n",
    "Pxx,fft_labels = powerspectra(rawdata,0,10,nbins=nb,binavg=False) #dataframe with power spectra for each axis\n",
    "Pxx.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = clip_data[4][sensor]['data'][0] #trial-sensor-clip#\n",
    "Pxx,fft_labels = powerspectra(rawdata,0,10,nbins=nb,binavg=False) #dataframe with power spectra for each axis\n",
    "Pxx.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Power spectra with Welch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal features\n",
    "rawdata = clip_data[0][sensor]['data'][0] #trial-sensor-clip#\n",
    "fig = plt.figure()\n",
    "# fp, ax_arr = plt.subplots(3, sharex=True)\n",
    "\n",
    "for i in range(3):\n",
    "    fig.add_subplot(3,1,i+1)\n",
    "    x = rawdata.iloc[:,i]\n",
    "    n = len(x) #number of samples in clip\n",
    "    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "    f,Pxx_den = welch(x,Fs,nperseg=256)\n",
    "    plt.plot(f,Pxx_den)\n",
    "#     plt.semilogy(f,Pxx_den)\n",
    "    plt.xlim([0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extraction(clip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data[1]['accel']['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate features data with scores for given task - cycle through all subjects\n",
    "for s in subjects:\n",
    "*    load score files\n",
    "*    load subject file\n",
    "*    choose task and sensor location\n",
    "*    extract clips\n",
    "*    compute features on each trial\n",
    "*    Aggregate subj code and score with feature matrix\n",
    "\n",
    "**Note: gyro data has to be added **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define task and sensor location\n",
    "task = 'Motor #8: Typing on a Computer'\n",
    "task_scores = 'Typing on a computer keyboard' #the task name in the scores file\n",
    "loc = 'dorsal_hand_right'\n",
    "\n",
    "#load subject scores\n",
    "mot_scores = pd.read_excel(os.path.join(scores_path, 'MotorTasks.xls'))\n",
    "#remove words:(Qxx) and 'rating' from each column for readability\n",
    "cols= mot_scores.columns\n",
    "cols = cols[4:]\n",
    "cols = cols.tolist()\n",
    "colsnew = [x.split('(')[0] for x in cols]\n",
    "colsnew = [x.strip() for x in colsnew] #remove whitspace\n",
    "colsnew = [x.split('rating')[0] for x in colsnew]\n",
    "colsnew = [x.strip() for x in colsnew]\n",
    "c = dict(zip(cols,colsnew))\n",
    "mot_scores = mot_scores.rename(index=str, columns=c)\n",
    "\n",
    "#load subjects features data and assemble with scores/subj metadata\n",
    "d = os.listdir(dict_path)\n",
    "fnames = [filename for filename in d if filename.startswith('1')]\n",
    "print(fnames)\n",
    "\n",
    "Data = pd.DataFrame() #the table with all data\n",
    "for subj_filename in fnames:\n",
    "\n",
    "    #extract current subject scores and metadata\n",
    "    subj = int(subj_filename[:4]) #subj code\n",
    "    subj_score = mot_scores.loc[mot_scores['Subject']==subj,['Subject','Visit',\n",
    "                                task_scores+ ' ' + 'bradykinesia right upper limb',\n",
    "                                task_scores+ ' ' + 'tremor right upper limb',\n",
    "                                task_scores+ ' ' + 'bradykinesia left upper limb',\n",
    "                                task_scores+ ' ' + 'tremor left upper limb']]\n",
    "    subj_score = subj_score.rename(index=str,columns=\n",
    "                                   {subj_score.columns[2]:'Bradykinesia right',subj_score.columns[3]:'Tremor right', \n",
    "                                 subj_score.columns[4]:'Bradykinesia left',subj_score.columns[5]:'Tremor left' })\n",
    "    subj_score.index = range(len(subj_score))\n",
    "    if len(subj_score) < 1:\n",
    "        print('no scores data for subject %d -- skipping..'%subj)\n",
    " \n",
    "    #load subject sensor data Pickle file (dictionary)\n",
    "    else:\n",
    "        f = open(os.path.join(dict_path,subj_filename),'rb')    \n",
    "        act_dict = pickle.load(f)\n",
    "        f.close()\n",
    "        print('\\nLoaded Subj %s sensor data'%subj)\n",
    "        \n",
    "        #high pass filter accelerometer data\n",
    "        HPfilter(act_dict,task,loc)\n",
    "\n",
    "        #generate clips and extract features\n",
    "        clip_data = gen_clips(act_dict,task=task,location=loc,overlap=0)\n",
    "        feature_extraction(clip_data)\n",
    "        \n",
    "        #aggreagate subject, scores and features data\n",
    "        n_visits = len(subj_score)    #of visits in Database\n",
    "        n_rec = len(clip_data.keys()) #of sensor recordings\n",
    "        print('n_visits = %d, # recordings = %d'%(n_visits,n_rec))\n",
    "        N = n_visits\n",
    "\n",
    "        if n_visits != n_rec:\n",
    "            print('# of recordings does not match # of visits! - matching first %d recordings'%(min([n_visits,n_rec])))\n",
    "            N = min([n_visits,n_rec])\n",
    "\n",
    "        #aggregate data from each visit for current subject\n",
    "        for i in range(N):\n",
    "            #features\n",
    "            D = clip_data[i]['accel']['features']\n",
    "            featcols = D.columns.tolist()\n",
    "            #scores\n",
    "            D['Bradykinesia right'] = subj_score['Bradykinesia right'][i]\n",
    "            D['Tremor right'] = subj_score['Tremor right'][i]\n",
    "            D['Bradykinesia left'] = subj_score['Bradykinesia left'][i]\n",
    "            D['Tremor left'] = subj_score['Tremor left'][i]\n",
    "            #metadata\n",
    "            D['Visit'] = subj_score.Visit[i] \n",
    "            D['Task'] = task\n",
    "            D['Location'] = loc \n",
    "            D['Subject'] = subj\n",
    "            Data = pd.concat([Data,D]) #concatenate data from each visit\n",
    "\n",
    "cols = ['Subject','Visit','Task','Location','Bradykinesia right','Bradykinesia left','Tremor right','Tremor left']+ featcols\n",
    "Data = Data[cols]    \n",
    "print('Data matrix generated')\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_score.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "fig.add_subplot(121)\n",
    "sns.distplot(Data['Bradykinesia left'],kde=False)\n",
    "sns.distplot(Data['Tremor left'],kde=False)\n",
    "plt.legend(['Bradyk','Tremor'])\n",
    "fig.add_subplot(122)\n",
    "sns.distplot(Data['Bradykinesia right'],kde=False)\n",
    "sns.distplot(Data['Tremor right'],kde=False)\n",
    "plt.legend(['Bradyk','Tremor'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on features to visualize subjects with bradykinesia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rows with symptoms\n",
    "indp = (Data['Bradykinesia left']>0) | (Data['Bradykinesia right']>0)\n",
    "indp = indp.astype(int) #1 = w bradykinesia\n",
    "indp = indp.values\n",
    "indp = indp.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize features\n",
    "X = Data.iloc[:,8:]\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nC = 10\n",
    "pca = PCA(n_components=nC)\n",
    "Xpca = pca.fit(X_std).transform(X_std)\n",
    "# Percentage of variance explained for each components\n",
    "print('total explained variance ratio (first %d components): %.3f'%(nC, pca.explained_variance_ratio_[0:nC].sum()))\n",
    "print(pca.explained_variance_ratio_[:10])\n",
    "plt.plot(range(1,nC+1),pca.explained_variance_ratio_,'.-')\n",
    "plt.xlabel('# of components')\n",
    "plt.ylabel('% of variance explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.scatter(Xpca[indp,0],Xpca[indp,1],cmap=plt.cm.Set1,edgecolors='k',alpha=0.4,label='No symptom')\n",
    "plt.scatter(Xpca[~indp,0],Xpca[~indp,1],cmap=plt.cm.Set1,edgecolors='k',alpha=1,label='Bradykinesia')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit an LDA to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "#TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on individual subjects / features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load subject scores\n",
    "#path = '../Scores/'\n",
    "mot_scores = pd.read_excel(os.path.join(scores_path, 'MotorTasks.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_scores[mot_scores['Subject']==1016].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1016\n",
    "#extract scores for corresponding task \n",
    "task = 'Motor #8: Typing on a Computer'\n",
    "loc = 'dorsal_hand_right'\n",
    "\n",
    "subj=1016\n",
    "subj_score = mot_scores.loc[mot_scores['Subject']==subj,['Subject','Visit',\n",
    "#                 'Typing on a computer keyboard overall score (Q92)',\n",
    "#                 'Typing on a computer keyboard bradykinesia left upper limb rating (Q93)',\n",
    "#                 'Typing on a computer keyboard tremor left upper limb rating (Q97)',\n",
    "                'Typing on a computer keyboard bradykinesia right upper limb rating (Q94)',\n",
    "                'Typing on a computer keyboard tremor right upper limb rating (Q98)']]\n",
    "\n",
    "subj_score = subj_score.rename(index=str,columns={subj_score.columns[2]:'Bradykinesia right',subj_score.columns[3]:'Tremor right' })\n",
    "subj_score.index = range(len(subj_score))\n",
    "subj_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggreagate subject, scores and features data\n",
    "Data = pd.DataFrame()\n",
    "D = pd.DataFrame()\n",
    "\n",
    "n_visits = len(subj_score)    # # of visits in Database\n",
    "n_rec = len(clip_data.keys()) # # of sensor recordings\n",
    "print('n_visits = %d, # recordings = %d'%(n_visits,n_rec))\n",
    "N = n_visits\n",
    "\n",
    "if n_visits != n_rec:\n",
    "    print('# of recordings does not match # of visits! - matching first %d recordings')%(min([n_visits,n_rec]))\n",
    "    N = min([n_visits,n_rec])\n",
    "\n",
    "for i in range(N):\n",
    "    #features\n",
    "    D = clip_data[i]['accel']['features']\n",
    "    featcols = D.columns.tolist()\n",
    "    #scores\n",
    "    D['Bradykinesia right'] = subj_score['Bradykinesia right'][i]\n",
    "    D['Tremor right'] = subj_score['Tremor right'][i]\n",
    "    #metadata\n",
    "    D['Visit'] = subj_score.Visit[i] \n",
    "    D['Task'] = task\n",
    "    D['Location'] = loc \n",
    "    Data = pd.concat([Data,D]) #concatenate data from each visit\n",
    "    \n",
    "Data['Subject'] = subj \n",
    "cols = ['Subject','Visit','Task','Location','Bradykinesia right','Tremor right']+ featcols\n",
    "Data = Data[cols]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_acc = clip_data[0]['accel']['features']\n",
    "Data_gyr = clip_data[0]['accel']['features']\n",
    "print(Data_acc.shape,Data_gyr.shape)\n",
    "Data = pd.concat([Data_acc,Data_gyr],axis=1)\n",
    "print(Data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict['Motor #10: Drinking Water'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks= ['Motor #13: Sitting','Motor #6: Sit to Stand','Motor #2: Walking','Motor #8: Typing on a Computer','Motor #4: Finger to Nose']\n",
    "trials = [0,5]\n",
    "# locs = ['dorsal_hand_right','flexor_digitorum_right','sacrum','anterior_thigh_right']\n",
    "# sensor = ['accel','gyro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = product(tasks,trials)\n",
    "taskslist = list(p)\n",
    "for t in taskslist:\n",
    "    plot_data(act_dict,t[0],t[1],'anterior_thigh_right','accel')\n",
    "    plt.title(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test itertools\n",
    "from itertools import product\n",
    "t = ('T1','T2')\n",
    "l = (1,2)\n",
    "s = ('s1','s2')\n",
    "# print(list(product(t,l,s)))\n",
    "taskslist = list(product(t,l,s))\n",
    "for t in taskslist:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(act_dict,'Motor #6: Sit to Stand',0,'sacrum','accel')\n",
    "plot_data(act_dict,'Motor #6: Sit to Stand',5,'sacrum','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'anterior_thigh_left','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'anterior_thigh_left','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'flexor_digitorum_right','accel')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'flexor_digitorum_right','accel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(act_dict,'Motor #6: Sit to Stand',0,'sacrum','gyro')\n",
    "plot_data(act_dict,'Motor #6: Sit to Stand',5,'sacrum','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'anterior_thigh_left','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'anterior_thigh_left','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',0,'flexor_digitorum_right','gyro')\n",
    "# plot_data(act_dict,'Motor #6: Sit to Stand',5,'flexor_digitorum_right','gyro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Subject 1020 - OFF\n",
    "plot_data(act_dict, 'Motor #7: Drawing on Paper', 0, 'dorsal_hand_right', 'accel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 1020 - ON\n",
    "plot_data(act_dict, 'Motor #7: Drawing on Paper', 5, 'dorsal_hand_right', 'accel', 100, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubID2 = '1029'\n",
    "\n",
    "act_dict2, timestamps2, testInfo2 = extract_data(SubID2, os.path.join(path, SubID2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 1029 - OFF\n",
    "plot_data(act_dict2, 'Motor #8: Typing on a Computer', 0, 'dorsal_hand_right', 'accel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject 1029 - ON\n",
    "plot_data(act_dict2, 'Motor #8: Typing on a Computer', 5, 'dorsal_hand_right', 'accel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_absent(df, error_df, testInfo):\n",
    "    Day1 = testInfo.loc[testInfo['Day'] == 'DAY 1', 'Date']\n",
    "    Day2 = testInfo.loc[testInfo['Day'] == 'DAY 2', 'Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps2.loc['Motor #6: Sit to Stand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path3 = r'C:\\Users\\andre\\Documents\\PD Study Data'\n",
    "errors = fix_errors(timestamps2, SubID2, path3)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestamps.loc['MDS-UPDRS #11: Rest Tremor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = fix_errors(timestamps, SubID, path3)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df = errors.loc[errors['Error'] == 'Absent']\n",
    "for i in range(0, int(error_df['Activity'].count())):\n",
    "    columns = ['EventType', 'Start Timestamp (ms)', 'Stop Timestamp (ms)', 'Cycle']\n",
    "    name = error_df.iloc[i]['Activity']\n",
    "    cycle = error_df.iloc[i]['Cycle']\n",
    "    temp_df = pd.DataFrame([name, 'NaN', 'NaN', cycle], index=columns).T.set_index('EventType')\n",
    "    timestamps = timestamps.append(temp_df)\n",
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
